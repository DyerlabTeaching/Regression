---
title: "Regression"
author: Rodney Dyer, PhD
format: revealjs
execute: 
  echo: true
---

## Linear Models 

The overall goal for our introduction here, is to develop models that may be parameterized by one or more terms.  The most simple model:


```{r setup, include=FALSE, echo=FALSE}
library( knitr )
library( reshape2 )
library( tidyverse )
library( kableExtra )
library( fontawesome )
knitr::opts_chunk$set( fig.retina = 3, 
                       warning = FALSE, 
                       message = FALSE,
                       fig.align="center")

theme_set( theme_minimal( base_size = 22) )
```



$$
y = \beta_0 + \beta_1 x_1 + \epsilon 
$$

which includes:  

- An intercept ( $\beta_0$ ),

- A slope coefficient ( $\beta_1$ ), and 

- And an error term ( $\epsilon$ ).







## Linear Models

```{r}
#| echo: false
df <- data.frame( x = 1:10,
                  y = c( 15.5, 28.1, 22.3, 32.3, 
                         31.1, 26.8, 41.8, 30.3, 
                         47.9, 41.1) )
ggplot( df, aes(x,y) ) + 
  stat_smooth( method="lm", 
               formula = y ~ x, 
               se=FALSE, 
               color="red", size=1) + 
  geom_point( size=4 ) 
```

# Building Models {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}




## Random Search {.smaller}

In the most fundamental form, we can make random searches

:::: {.columns}

::: {.column width="50%"}
```{r}
models <- data.frame( beta0 = runif(250,-20,40),
                      beta1 = runif(250, -5, 5))
summary( models )
```
:::

::: {.column width="50%"}
::: {.fragment}
```{r echo=FALSE}
ggplot() + 
  geom_abline( aes(intercept = beta0, 
                   slope = beta1), 
               data = models,
               alpha = 0.2) + 
  geom_point( aes(x,y), 
              data=df, 
              size = 4 )
```
:::
:::

::::



## Building A Model - Search Criterion

```{r echo=FALSE}
fit <- lm( y ~ x, data=df )
yhat <- data.frame( x = 1:10,
                    yhat = predict( fit ),
                    y= df$y )

ggplot( df ) + 
  geom_abline( aes( intercept = fit$coefficients[1],
                    slope = fit$coefficients[2]),
               color="red",
               size = 1.25 ) +
  geom_segment( aes(x=x, y = y, xend = x, yend = yhat), 
                data = yhat,
                color = "blue", size = 1.25) + 
  geom_point( aes(x,y), size=4 ) 
```






## Searching Random Model Space

```{r}
model_distance <- function( interscept, slope, X, Y ) {
  yhat <- interscept + slope * X
  diff <- Y - yhat
  return( sqrt( mean( diff ^ 2 ) ) )
}
```

## Searching Random Model Space

```{r}
models$dist <- NA
for( i in 1:nrow(models) ) {
  models$dist[i] <- model_distance( models$beta0[i],
                                    models$beta1[i],
                                    df$x,
                                    df$y )
}
head( models )
```



## Top 10 Random Models {.smaller}

:::: {.columns}

::: {.column width="50%"}

Take the 10 models with the smallest sum of squared distances.

```{r echo=FALSE}
ggplot()  + 
  geom_abline( aes(intercept = beta0,
                   slope = beta1, 
                   color = -dist),
               data = filter( models, rank(dist) <= 10 ),
               alpha = 0.5) + 
  geom_point( aes(x,y),
              data=df, size=2) 
```
:::

::: {.column width="50%"}
```{r eval=FALSE}
ggplot()  + 
  geom_abline( aes(intercept = beta0,
                   slope = beta1, 
                   color = -dist),
               data = filter( models, 
                              rank(dist) <= 10 ),
               alpha = 0.5) + 
  geom_point( aes(x,y),
              data=df, 
              size=2) 
```

:::

::::








## The Best Coefficients

```{r fig.height=5}
ggplot( models, aes(x = beta0, y = beta1, color = -dist)) + 
  geom_point( data = filter( models, rank(dist) <= 10), 
              color = "red",  size = 4) + 
  geom_point()
```



## Systematic Grid Search {.smaller}

We could then systematically search the $\beta_0$, $\beta_1$ response space for the best set of coefficients.


:::: {.columns}

::: {.column width="50%"}
```{r echo=FALSE}
grid <- expand.grid( beta0 = seq(15, 
                                 20, 
                                 length = 25),
                     beta1 = seq(2, 
                                 3, 
                                 length = 25))
grid$dist <- NA
for( i in 1:nrow(grid) ) {
  grid$dist[i] <- model_distance( grid$beta0[i],
                                  grid$beta1[i],
                                  df$x,
                                  df$y )
}

ggplot( grid, aes(x = beta0, 
                  y = beta1,
                  color = -dist)) + 
  geom_point( data = filter( grid, 
                             rank(dist) <= 10), 
              color = "red",
              size = 4) +
  geom_point()
```
:::

::: {.column width="50%"}
```{r eval=FALSE}
grid <- expand.grid( beta0 = seq(15, 
                                 20, 
                                 length = 25),
                     beta1 = seq(2, 
                                 3, 
                                 length = 25))
grid$dist <- NA
for( i in 1:nrow(grid) ) {
  grid$dist[i] <- model_distance( grid$beta0[i],
                                  grid$beta1[i],
                                  df$x,
                                  df$y )
}

ggplot( grid, aes(x = beta0, 
                  y = beta1,
                  color = -dist)) + 
  geom_point( data = filter( grid, 
                             rank(dist) <= 10), 
              color = "red",
              size = 4) +
  geom_point()
```
:::

::::




# Our Friend Least Squares Regression {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}


## &nbsp; 

![](https://live.staticflickr.com/65535/50588297022_62f043a616_c_d.jpg)




## Specifying a Formula

For `R` to perform these analyses for us, we need to be able to specify the *function* that we will use to represent the data (just like we did for `plot(y~x)`).  Assuming the predictor is named `x` and the response is `y`, we have:


:::{.fragment}
*Single Predictor Model*

```
y ~ x
```
:::


:::{.fragment}
*Multiple Additive Predictors*

```
y ~ x1 + x2 
```
:::

:::{.fragment}
*Interaction Terms*

```
y ~ x1 + x2 + x1*x2
```
:::


## Fitting A Model

The easiest model.  

```{r}
fit <- lm( y ~ x, data = df )
fit 
```



## Model Summaries

Summaries of the overal model components.

```{r}
summary( fit )
```


## Components within the Summary Object

Like `cor.test()`, the object returned from `lm()` and its summary object have several internal components that you may use.

```{r}
names( fit )
```

&nbsp;

:::{.fragment}
```{r}
names( summary( fit ) )
```
:::




## Components within the Summary Object

The probability can be found by looking at the data in the `F-Statistic` and then asking the F-distribution for the probability associated with the value of the test statistic and the degrees of freedom for both the model and the residuals.

```{r}
summary( fit )$fstatistic 
```

## The Summary Object

When you print out the `summary( fit )` object, it uses the `fstatistic` to estimate a `P-value`.  If we need that P-value directly, this is how it is done.

```{r}
get_pval <- function( model ) {
  f <- summary( model )$fstatistic[1]
  df1 <- summary( model )$fstatistic[2]
  df2 <- summary( model )$fstatistic[3]
  p <- as.numeric( 1.0 - pf( f, df1, df2 ) )
  return( p  )
}

get_pval( fit )
```












# So You Have a Model, Now What? {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}





## Model Diagnostics - Residuals

```{r fig.height=5}
plot( fit, which = 1 )
```







## Normality Of the Data

```{r fig.height=5}
plot( fit, which = 2 )
```







## Leverage

```{r fig.height=5}
plot( fit, which=5 )
```






## Decomposition of Variance

The terms in this table are:  

- Degrees of Freedom (*df*): representing `1` degree of freedom for the model, and `N-1` for the residuals.   



## Decomposition of Variance

The terms in this table are:  

- Sums of Squared Deviations: 
    - $SS_{Total} = \sum_{i=1}^N (y_i - \bar{y})^2$
    - $SS_{Model} = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2$, and 
    - $SS_{Residual} = SS_{Total} - SS_{Model}$
    


    
## Decomposition of Variance

The terms in this table are:  

- Mean Squares (Standardization of the Sums of Squares for the degrees of freedom)  
    - $MS_{Model} = \frac{SS_{Model}}{df_{Model}}$
    - $MS_{Residual} = \frac{SS_{Residual}}{df_{Residual}}$






## Decomposition of Variance

The terms in this table are:  
    
- The $F$-statistic is from a known distribution and is defined by the ratio of Mean Squared values.

- `Pr(>F)` is the probability associated the value of the $F$-statistic and is dependent upon the degrees of freedom for the model and residuals.





## Decomposition of Variance

The classis ANOVA Table

```{r}
anova( fit )
```


## Variance Exaplained

```{r}
summary( fit )
```





## Relationship Between $R^2$ & $r$

How much of the variation is explained?


$$
R^2 = \frac{SS_{Model}}{SS_{Total}}
$$

:::{.fragment}

```{r}
c( `Regression R^2` = summary( fit )$r.squared,
   `Squared Correlation` = as.numeric( cor.test( df$x, df$y )$estimate^2 ) )
```

> The square of the Pearson Correlation is equal to R

:::


## Helper Functions

Grabbing the predicted values $\hat{y}$ from the model.

```{r}
df$yhat <- predict( fit )
df %>% 
  ggplot( aes(x,yhat) ) + geom_line() + geom_point() +
  geom_point( aes(y=y), col="darkred", size=2)
```


## Helper Functions - Residuals

The residuals are the distances between the observed value and its corresponding value on the fitted line.

```{r}
df$residuals <- residuals( fit ) 
df %>%
  ggplot( aes(yhat, residuals) ) + 
  geom_point() + 
  geom_abline( slope=0,intercept=0, color="red")
```




































# Comparing Models {background-color="black" background-image="media/contour.png" background-size="initial" background-position="right"}



## What Makes One Model Better

There are two parameters that we have already looked at that may help.  These are:

- The $P-value$: Models with smaller probabilities could be considered more informative.  

- The $R^2$: Models that explain more of the variation may be considered more informative.




## New Data Set

Let's start by looking at some air quality data, that is built into `R` as an example data set.

```{r}
airquality %>%
  select( -Month, -Day ) -> df.air
summary( df.air )
```






## Base Models - What Influences Ozone

Individually, we can estimate a set of first-order models.   

$$
y = \beta_0 + \beta_1 x_1 + \epsilon
$$

```{r}
fit.solar <- lm( Ozone ~ Solar.R, data = df.air )
fit.temp <- lm( Ozone ~ Temp, data = df.air )
fit.wind <- lm( Ozone ~ Wind, data = df.air )
```


## Base Models - What Influences Ozone

```{r echo=FALSE}
data.frame( Model = c( "Ozone ~ Solar",
                       "Ozone ~ Temp",
                       "Ozone ~ Wind"), 
            R2 = c( summary( fit.solar )$r.squared,
                    summary( fit.temp )$r.squared,
                    summary( fit.wind )$r.squared ), 
            P = c( get_pval( fit.solar), 
                   get_pval( fit.temp ),
                   get_pval( fit.wind ) ) ) -> df.models

df.models %>%
  mutate( P = format( P, scientific=TRUE, digits=3)) %>%
  kable( caption = "Model parameters predicting mean ozone in parts per billion mearsured in New York during the period of 1 May 1973 - 30 September 1973 as predicted by Temperature, Windspeed, and Solar Radiation.",
         digits = 3,
         align='l') %>%
  kable_classic_2()
```




## More Complicated Models

Multiple Regression Model - Including more than one predictors.

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$



```{r}
fit.temp.wind <- lm( Ozone ~ Temp + Wind, data = df.air )
fit.temp.solar <- lm( Ozone ~ Temp + Solar.R, data = df.air )
fit.wind.solar <- lm( Ozone ~ Wind + Solar.R, data = df.air )
```


## More Complicated Models

```{r echo=FALSE}
df.models <- rbind( df.models, 
                    data.frame( Model = c( "Ozone ~ Temp + Wind",
                                           "Ozone ~ Temp + Solar",
                                           "Ozone ~ Wind + Solar" ),
                                R2 = c( summary( fit.temp.wind )$r.squared,
                                        summary( fit.temp.solar )$r.squared,
                                        summary( fit.wind.solar )$r.squared ),
                                P = c( get_pval( fit.temp.wind),
                                       get_pval( fit.temp.solar),
                                       get_pval( fit.wind.solar) )
                                ))
df.models %>%
  mutate( P = format( P, scientific=TRUE, digits=3)) %>%
  kable( caption = "Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 1973 - 30 September 1973.",
         digits = 3,
         align="l") %>%
  kable_classic_2()
```



## For Completeness

How about all the predictors. $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon$

```{r}
fit.all <- lm( Ozone ~ Solar.R + Temp + Wind, data = df.air )
```

## For Completeness

```{r echo=FALSE}
df.models <- rbind( df.models, 
                    data.frame( Model = c( "Ozone ~ Temp + Wind + Solar"),
                                R2 = c( summary( fit.all )$r.squared ),
                                P = c( get_pval( fit.all)  )
                                ))


df.models$P = cell_spec( format( df.models$P, 
                                 digits=3, 
                                 scientific=TRUE), 
                         color = ifelse( df.models$P == min(df.models$P), 
                                         "red",
                                         "black"))
df.models$R2 = cell_spec( format( df.models$R2, 
                                  digits=3, 
                                  scientific=TRUE), 
                          color = ifelse( df.models$R2 == max( df.models$R2), 
                                          "green",
                                          "black"))

df.models %>%
  mutate( P = format( P, digits=3, scientific = TRUE) ) %>% 
  kable( escape = FALSE) %>%
  kable_paper( "striped", full_width = FALSE )
```



## $R^2$ Inflation

Any variable added to a model will be able to generate *Sums of Squares* (even if it is a small amount).  So, `adding variables may artifically inflate the Model Sums of Squares`.


Example:

> What happens if I add just random data to the regression models?  How does $R^2$ change?



## Random Data Effects

```{r echo=FALSE}
random.models  <- list()
random.models[["Ozone ~ Temp"]] <- fit.temp
random.models[["Ozone ~ Wind"]] <- fit.wind
random.models[["Ozone ~ Solar"]] <- fit.solar
random.models[["Ozone ~ Temp + Wind"]] <- fit.temp.wind
random.models[["Ozone ~ Temp + Solar"]] <- fit.temp.solar
random.models[["Ozone ~ Wind + Solar"]] <- fit.wind.solar
random.models[[ "Ozone ~ Temp + Wind + Solar" ]] <- fit.all

df.tmp <- df.air

for( i in 1:8 ) {
  lbl <- paste("Ozone ~ Temp + Wind + Solar + ", i, " Random Variables", sep="")
  df.tmp[[lbl]] <- runif( nrow(df.tmp), min = -100, max = 100 )
  random.models[[lbl]] <- lm( Ozone ~ ., data = df.tmp ) 
}

data.frame( Models = names( random.models ),
            R2 = sapply( random.models, 
                          FUN = function( x ) return( summary( x )$r.squared), 
                          simplify = TRUE ),
            P = sapply( random.models, 
                        FUN = get_pval ) ) -> df.random
```


```{r echo=FALSE}
df.random %>%
  head( n = 7 ) %>%
  select( Models, R2 ) %>%
  kable( caption = "Original data in models.",
         digits=4,
         row.names = FALSE,
         align="l") %>%
  kable_paper("striped", full_width = FALSE )
```

## Random Data Effects

```{r echo=FALSE}
df.random %>%
  tail( n = 8 ) %>%
  select( Models, R2 ) %>%
  kable( caption = "Original full model + X random variables",
         digits=4,
         row.names = FALSE,
         align='l') %>%
  kable_paper("striped", full_width = FALSE )
```







## Perfect - My Models RULE

I can just add **random** variables to my model and always get an <font color="red">awesome</font> fit!

<p>&nbsp;</p>

<center>
![](https://media.giphy.com/media/PFT49iGCp0FBm/giphy.gif)

</center>

<p>&nbsp;</p>

Not so fast Bevis!







## Model Comparisons

Akaike Information Criterion (AIC) is a measurement that allows us to compare models while penalizing for adding new parameters.

$AIC = -2 \ln L + 2p$

The criterion here are to find models with the lowest AIC values.


:::{.fragment}
### Comparisons

To compare, we evaluate the differences in AIC for alternative models.

$\delta AIC = AIC - min( AIC )$
:::







## AIC & âˆ‚AIC {.smaller .scrollable}

```{r echo=FALSE}
df.random$AIC <- sapply( random.models, 
                         FUN = AIC, 
                         simplify = TRUE )

df.random %>%
  mutate( deltaAIC = AIC - min(AIC) ) %>%
  select( -P ) %>%
  kable( escape = FALSE,
         row.names = FALSE, 
         digits = 3) %>%
  kable_paper( "striped", full_width = FALSE )
```













































































## Questions

::: {layout-ncol="2"}
If you have any questions, please feel free to either post them as an "Issue" on your copy of this GitHub Repository, post to the [Canvas](https://canvas.vcu.edu) discussion board for the class, or drop me an [email](mailto://rjdyer@vcu.edu).

![](media/peter_sellers.gif){.middle fig-alt="Peter Sellers looking bored" fig-align="center" width="500"}
:::
